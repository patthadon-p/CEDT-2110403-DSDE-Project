# # import datetime
# # import streamlit as st
# # import pandas as pd

# # st.set_page_config(layout="wide")

# # st.title("Bangkok Traffy ‚Äì Scatter Only Viewer")
# # st.sidebar.header("Filters")

# # # Load Cleansed Data
# # @st.cache_data
# # def load_data():
# #     df = pd.read_csv("./data/processed/cleansed_data.csv")
# #     df["type_cleaned"] = (
# #         df["type"]
# #         .astype(str)
# #         .str.replace("{", "")
# #         .str.replace("}", "")
# #         .str.split(",")
# #     )
# #     return df

# # df_cleansed = load_data()

# # # Load Type list
# # @st.cache_data
# # def get_type_list(df):
# #     return sorted({t.strip() for row in df["type_cleaned"] for t in row})

# # type_list = get_type_list(df_cleansed)

# # # Sidebar Filters
# # with st.sidebar.form("filter_form"):

# #     type_filter = st.selectbox(
# #         "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
# #         options=["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"] + type_list
# #     )

# #     start_date, end_date = st.date_input(
# #         "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô",
# #         value=[
# #             datetime.date(2021, 9, 19),
# #             datetime.date(2025, 1, 16)
# #         ],
# #         min_value=datetime.date(2021, 9, 19),
# #         max_value=datetime.date(2025, 1, 16)
# #     )

# #     submit = st.form_submit_button("Apply Filter")

# # if submit:
# #     st.session_state["type_filter"] = type_filter
# #     st.session_state["start_date"] = start_date
# #     st.session_state["end_date"] = end_date

# # type_filter = st.session_state.get("type_filter", "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
# # start_date = st.session_state.get("start_date", datetime.date(2021, 9, 19))
# # end_date = st.session_state.get("end_date", datetime.date(2025, 1, 16))

# # # ----- filter by time -----
# # filtered_time = df_cleansed[
# #     (df_cleansed[["timestamp_year", "timestamp_month", "timestamp_date"]]
# #         .apply(tuple, axis=1) >= (start_date.year, start_date.month, start_date.day))
# #     &
# #     (df_cleansed[["timestamp_year", "timestamp_month", "timestamp_date"]]
# #         .apply(tuple, axis=1) <= (end_date.year, end_date.month, end_date.day))
# # ]

# # # ----- filter by type -----
# # if type_filter != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î":
# #     gdf_filtered = filtered_time[filtered_time["type_clean"] == type_filter]
# # else:
# #     gdf_filtered = filtered_time
# #     # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ text ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô None
# #     type_filter = ""

# # # NEW: prepare daily counts for scatter chart
# # daily_counts = (
# #     gdf_filtered
# #     .groupby(["timestamp_year", "timestamp_month", "timestamp_date"])
# #     .size()
# #     .reset_index(name="count")
# # )

# # if daily_counts.empty:
# #     st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
# # else:
# #     daily_counts["date"] = pd.to_datetime(
# #         daily_counts[["timestamp_year", "timestamp_month", "timestamp_date"]]
# #         .rename(columns={
# #             "timestamp_year": "year",
# #             "timestamp_month": "month",
# #             "timestamp_date": "day"
# #         })
# #     )

# #     # scatter chart section
# #     st.markdown("---")
# #     st.subheader(f"üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ {type_filter if type_filter else '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'} ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤ (Scatter Chart)")

# #     st.scatter_chart(
# #         daily_counts,
# #         x="date",
# #         y="count"
# #     )

# import datetime
# import streamlit as st
# import pandas as pd
# import altair as alt   # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Altair ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤

# st.set_page_config(layout="wide")

# st.title("Bangkok Traffy ‚Äì Scatter Only Viewer")
# st.sidebar.header("Filters")

# # Load Cleansed Data
# @st.cache_data
# def load_data():
#     df = pd.read_csv("./data/processed/cleansed_data.csv")
#     df["type_cleaned"] = (
#         df["type"]
#         .astype(str)
#         .str.replace("{", "")
#         .str.replace("}", "")
#         .str.split(",")
#     )
#     return df

# df_cleansed = load_data()

# # Load Type list
# @st.cache_data
# def get_type_list(df):
#     return sorted({t.strip() for row in df["type_cleaned"] for t in row})

# type_list = get_type_list(df_cleansed)

# # Sidebar Filters
# with st.sidebar.form("filter_form"):

#     type_filter = st.selectbox(
#         "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
#         options=["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"] + type_list
#     )

#     start_date, end_date = st.date_input(
#         "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô",
#         value=[
#             datetime.date(2021, 9, 19),
#             datetime.date(2025, 1, 16)
#         ],
#         min_value=datetime.date(2021, 9, 19),
#         max_value=datetime.date(2025, 1, 16)
#     )

#     submit = st.form_submit_button("Apply Filter")

# if submit:
#     st.session_state["type_filter"] = type_filter
#     st.session_state["start_date"] = start_date
#     st.session_state["end_date"] = end_date

# type_filter = st.session_state.get("type_filter", "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
# start_date = st.session_state.get("start_date", datetime.date(2021, 9, 19))
# end_date = st.session_state.get("end_date", datetime.date(2025, 1, 16))

# # ----- filter by time -----
# filtered_time = df_cleansed[
#     (df_cleansed[["timestamp_year", "timestamp_month", "timestamp_date"]]
#         .apply(tuple, axis=1) >= (start_date.year, start_date.month, start_date.day))
#     &
#     (df_cleansed[["timestamp_year", "timestamp_month", "timestamp_date"]]
#         .apply(tuple, axis=1) <= (end_date.year, end_date.month, end_date.day))
# ]

# # ----- filter by type -----
# if type_filter != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î":
#     gdf_filtered = filtered_time[filtered_time["type_clean"] == type_filter]
# else:
#     gdf_filtered = filtered_time
#     # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ text ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô None
#     type_filter = ""

# # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° daily counts
# daily_counts = (
#     gdf_filtered
#     .groupby(["timestamp_year", "timestamp_month", "timestamp_date"])
#     .size()
#     .reset_index(name="count")
# )

# if daily_counts.empty:
#     st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
# else:
#     daily_counts["date"] = pd.to_datetime(
#         daily_counts[["timestamp_year", "timestamp_month", "timestamp_date"]]
#         .rename(columns={
#             "timestamp_year": "year",
#             "timestamp_month": "month",
#             "timestamp_date": "day"
#         })
#     )

#     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå year_month ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏™‡∏µ (optional)
#     daily_counts["year_month"] = daily_counts["date"].dt.to_period("M").astype(str)

#     st.markdown("---")
#     st.subheader(f"üìà ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ {type_filter if type_filter else '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'} ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤ (Altair Scatter)")

#     # ‚úÖ Altair chart: line + scatter + tooltip + zoom/pan
#     base = alt.Chart(daily_counts).encode(
#         x=alt.X("date:T", title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"),
#         y=alt.Y("count:Q", title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤"),
#         tooltip=["date:T", "count:Q", "year_month:N"]
#     )

#     line = base.mark_line(opacity=0.6)
#     points = base.mark_circle(size=60, opacity=0.8).encode(
#         color=alt.Color("year_month:N", title="‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
#     )

#     chart = (line + points).interactive()

#     st.altair_chart(chart, use_container_width=True)

#     # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
#     st.dataframe(
#         daily_counts[["date", "count", "year_month"]]
#         .sort_values("date")
#     )
import datetime

import pandas as pd
import streamlit as st

st.set_page_config(layout="wide")
st.title("Bangkok Traffy ‚Äì Scatter Insights Dashboard")
st.sidebar.header("Filters")

# -----------------------------
# 1) LOAD & PREPROCESS DATA
# -----------------------------
@st.cache_data
def load_data():
    df = pd.read_csv("./data/processed/cleansed_data.csv")

    # type_cleaned: list of categories per ticket
    if "type_cleaned" not in df.columns:
        df["type_cleaned"] = (
            df["type"]
              .astype(str)
              .str.replace("{", "")
              .str.replace("}", "")
              .str.split(",")
        )

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ type_clean ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô single label, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏à‡∏≤‡∏Å type_cleaned
    if "type_clean" not in df.columns:
        df["type_clean"] = df["type_cleaned"].apply(lambda xs: xs[0].strip() if len(xs) > 0 else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏")

    # ------------------
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp ‡∏à‡∏£‡∏¥‡∏á
    # ------------------
    if {"timestamp_year", "timestamp_month", "timestamp_date"}.issubset(df.columns):
        df["timestamp_dt"] = pd.to_datetime(
            df[["timestamp_year", "timestamp_month", "timestamp_date"]]
            .rename(columns={
                "timestamp_year": "year",
                "timestamp_month": "month",
                "timestamp_date": "day"
            }),
            errors="coerce"
        )
    elif "timestamp" in df.columns:
        df["timestamp_dt"] = pd.to_datetime(df["timestamp"], errors="coerce")
    else:
        st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp_year/month/date ‡∏´‡∏£‡∏∑‡∏≠ timestamp ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå cleansed_data.csv")
        df["timestamp_dt"] = pd.NaT

    # hour / day / month / year
    df["hour"] = df["timestamp_dt"].dt.hour
    df["day_of_month"] = df["timestamp_dt"].dt.day
    df["month"] = df["timestamp_dt"].dt.month
    df["year"] = df["timestamp_dt"].dt.year
    df["date"] = df["timestamp_dt"].dt.date

    # ------------------
    # last_activity + resolve_time (‡∏ä‡∏°.)
    # ------------------
    if "last_activity" in df.columns:
        df["last_activity_dt"] = pd.to_datetime(df["last_activity"], errors="coerce")
        df["resolve_hours"] = (df["last_activity_dt"] - df["timestamp_dt"]).dt.total_seconds() / 3600.0
    else:
        df["last_activity_dt"] = pd.NaT
        df["resolve_hours"] = pd.NA

    # star rating
    if "star" in df.columns:
        df["star"] = pd.to_numeric(df["star"], errors="coerce")

    # count_reopen
    if "count_reopen" in df.columns:
        df["count_reopen"] = pd.to_numeric(df["count_reopen"], errors="coerce")

    # TODO: ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå lat/lon ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ columns ‡∏ä‡∏∑‡πà‡∏≠ lat, lon
    if "lat" not in df.columns or "lon" not in df.columns:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á dummy ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô (‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ scatter ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÜ)
        df["lat"] = pd.NA
        df["lon"] = pd.NA

    # district / subdistrict ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if "district" not in df.columns and "district_name" in df.columns:
        df["district"] = df["district_name"]

    return df

df = load_data()

# -----------------------------
# 2) SIDEBAR FILTERS
# -----------------------------
# type list
type_list = sorted({t.strip() for row in df["type_cleaned"] for t in row})

type_filter = st.sidebar.selectbox(
    "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
    options=["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"] + type_list
)

# district filter (optional)
district_options = ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"]
if "district" in df.columns:
    district_options += sorted(df["district"].dropna().unique().tolist())

district_filter = st.sidebar.selectbox(
    "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ç‡∏ï (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)",
    options=district_options
)

# date range
min_date = df["date"].min() or datetime.date(2021, 1, 1)
max_date = df["date"].max() or datetime.date(2025, 12, 31)

start_date, end_date = st.sidebar.date_input(
    "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô",
    value=[min_date, max_date],
    min_value=min_date,
    max_value=max_date,
)

# -----------------------------
# 3) APPLY FILTERS
# -----------------------------
filtered = df.copy()

# date filter
filtered = filtered[
    (filtered["date"] >= start_date) &
    (filtered["date"] <= end_date)
]

# type filter
if type_filter != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î":
    filtered = filtered[filtered["type_clean"] == type_filter]

# district filter
if district_filter != "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î" and "district" in filtered.columns:
    filtered = filtered[filtered["district"] == district_filter]

st.write(
    f"‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î **{len(filtered)}** ‡πÄ‡∏Ñ‡∏™ "
    f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: **{type_filter if type_filter != '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î' else '‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó'}** "
    f"{' | ‡πÄ‡∏Ç‡∏ï: ' + district_filter if district_filter != '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î' else ''}"
)

if filtered.empty:
    st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
    st.stop()

# -----------------------------
# 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
# -----------------------------
# daily counts
daily_counts = (
    filtered
    .groupby("date")
    .size()
    .reset_index(name="count")
    .sort_values("date")
)
daily_counts["date_dt"] = pd.to_datetime(daily_counts["date"])

# rolling mean & std ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ anomaly detection
daily_counts["roll_mean_7"] = daily_counts["count"].rolling(window=7, min_periods=1).mean()
daily_counts["roll_std_7"] = daily_counts["count"].rolling(window=7, min_periods=1).std()
daily_counts["roll_std_7"].fillna(0, inplace=True)
daily_counts["is_anomaly"] = daily_counts["count"] > (daily_counts["roll_mean_7"] + 2 * daily_counts["roll_std_7"])

# -----------------------------
# 5) TABS
# -----------------------------
tab_time, tab_area, tab_problem, tab_perf, tab_anom = st.tabs(
    ["‚è∞ Time Patterns", "üìç Area / Location", "üß© Problem Types", "‚öôÔ∏è Performance", "üö® Anomalies"]
)

# ============================
# TAB A: TIME PATTERNS
# ============================
with tab_time:
    st.subheader("A1) Scatter: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (Hour-of-Day)")

    hour_counts = (
        filtered
        .groupby("hour")
        .size()
        .reset_index(name="count")
        .sort_values("hour")
    )

    st.scatter_chart(hour_counts, x="hour", y="count")
    st.caption("‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ß‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î")

    st.markdown("---")
    st.subheader("A2) Scatter: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (Day-of-Month)")

    dom_counts = (
        filtered
        .groupby("day_of_month")
        .size()
        .reset_index(name="count")
        .sort_values("day_of_month")
    )
    st.scatter_chart(dom_counts, x="day_of_month", y="count")
    st.caption("‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏Ç‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡∏ö‡πà‡∏≠‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡πâ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏õ‡∏•‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏Ø‡∏•‡∏Ø)")

    st.markdown("---")
    st.subheader("A3) Scatter + Trend: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô")

    st.scatter_chart(daily_counts, x="date_dt", y="count")
    st.line_chart(daily_counts.set_index("date_dt")[["roll_mean_7"]])
    st.caption("‡∏à‡∏∏‡∏î = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô, ‡πÄ‡∏™‡πâ‡∏ô = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7 ‡∏ß‡∏±‡∏ô (‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏á)")

# ============================
# TAB B: AREA / LOCATION
# ============================
with tab_area:
    st.subheader("B1) Scatter: ‡∏û‡∏¥‡∏Å‡∏±‡∏î Lat/Lon (Colored by Month)")

    loc_df = filtered.dropna(subset=["lat", "lon"]).copy()
    if loc_df.empty:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• lat/lon ‡πÉ‡∏ô dataset ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå lat / lon")
    else:
        # month label
        loc_df["month_label"] = loc_df["timestamp_dt"].dt.strftime("%Y-%m")
        # ‡πÉ‡∏ä‡πâ scatter_chart ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ color ‡πÄ‡∏õ‡πá‡∏ô month_label
        st.scatter_chart(
            loc_df,
            x="lon",
            y="lat",
            color="month_label",
        )
        st.caption("‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏à‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏û‡∏¥‡∏Å‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏µ‡πÅ‡∏ó‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏à‡πâ‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤")

    st.markdown("---")
    st.subheader("B2) Scatter: ‡πÄ‡∏Ç‡∏ï vs ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ä‡∏°.)")

    if "district" not in filtered.columns or filtered["resolve_hours"].dropna().empty:
        st.info("‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå district ‡πÅ‡∏•‡∏∞ resolve_hours (‡∏à‡∏≤‡∏Å timestamp + last_activity) ‡∏à‡∏∂‡∏á‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ")
    else:
        district_resolve = (
            filtered
            .dropna(subset=["resolve_hours"])
            .groupby("district")["resolve_hours"]
            .mean()
            .reset_index(name="avg_resolve_hours")
            .sort_values("avg_resolve_hours")
        )
        st.scatter_chart(district_resolve, x="district", y="avg_resolve_hours")
        st.caption("‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡∏ï‡πÑ‡∏´‡∏ô‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß / ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡∏ï‡∏≠‡∏∑‡πà‡∏ô (‡∏´‡∏ô‡πà‡∏ß‡∏¢: ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)")

# ============================
# TAB C: PROBLEM TYPES
# ============================
with tab_problem:
    st.subheader("C1) Scatter: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™ vs ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏õ‡∏±‡∏ç‡∏´‡∏≤)")

    if filtered["resolve_hours"].dropna().empty:
        st.info("‡∏¢‡∏±‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì resolve_hours ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡∏°‡∏µ last_activity) ‡πÄ‡∏•‡∏¢‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ")
    else:
        type_stats = (
            filtered
            .dropna(subset=["resolve_hours"])
            .groupby("type_clean")
            .agg(
                total_cases=("type_clean", "size"),
                avg_resolve_hours=("resolve_hours", "mean"),
            )
            .reset_index()
        )
        st.scatter_chart(
            type_stats,
            x="total_cases",
            y="avg_resolve_hours",
        )
        st.caption("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô = ‡πÄ‡∏Ñ‡∏™‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡∏ä‡πâ‡∏≤ ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡πà‡∏≠‡∏ô")

    st.markdown("---")
    st.subheader("C2) Scatter: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™ vs ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏î‡∏≤‡∏ß (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à)")

    if "star" not in filtered.columns or filtered["star"].dropna().empty:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå star ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏£‡∏ï‡∏ï‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠")
    else:
        type_star = (
            filtered
            .dropna(subset=["star"])
            .groupby("type_clean")
            .agg(
                total_cases=("type_clean", "size"),
                avg_star=("star", "mean"),
            )
            .reset_index()
        )
        st.scatter_chart(
            type_star,
            x="total_cases",
            y="avg_star",
        )
        st.caption("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏™‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥ = ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏™‡∏π‡∏á")

# ============================
# TAB D: PERFORMANCE
# ============================
with tab_perf:
    st.subheader("D1) Scatter: ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ vs ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å reopen")

    if "count_reopen" not in filtered.columns or filtered[["resolve_hours", "count_reopen"]].dropna().empty:
        st.info("‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå resolve_hours ‡πÅ‡∏•‡∏∞ count_reopen ‡∏à‡∏∂‡∏á‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ")
    else:
        perf_df = filtered.dropna(subset=["resolve_hours", "count_reopen"]).copy()
        st.scatter_chart(
            perf_df,
            x="resolve_hours",
            y="count_reopen"
        )
        st.caption("‡∏ñ‡πâ‡∏≤‡∏à‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏∏‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏Å‡πâ‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡∏∞ reopen ‡∏ö‡πà‡∏≠‡∏¢ ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç")

    st.markdown("---")
    st.subheader("D2) Scatter: ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ vs ‡πÄ‡∏Ç‡∏ï")

    if "district" not in filtered.columns or filtered["resolve_hours"].dropna().empty:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ district ‡∏´‡∏£‡∏∑‡∏≠ resolve_hours ‡πÑ‡∏°‡πà‡∏û‡∏≠")
    else:
        district_perf = (
            filtered
            .dropna(subset=["resolve_hours"])
            .groupby("district")["resolve_hours"]
            .mean()
            .reset_index(name="avg_resolve_hours")
        )
        st.scatter_chart(
            district_perf,
            x="district",
            y="avg_resolve_hours"
        )
        st.caption("‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡∏ï‡πÅ‡∏ö‡∏ö high-level")

# ============================
# TAB E: ANOMALIES
# ============================
with tab_anom:
    st.subheader("E1) Scatter: Anomaly Detection ‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô")

    if daily_counts.empty:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠")
    else:
        # ‡πÅ‡∏¢‡∏Å 2 ‡∏Å‡∏•‡∏∏‡πà‡∏°: ‡∏õ‡∏Å‡∏ï‡∏¥ vs anomaly
        normal_days = daily_counts[~daily_counts["is_anomaly"]]
        anomaly_days = daily_counts[daily_counts["is_anomaly"]]

        st.write("‡∏à‡∏∏‡∏î‡∏™‡∏µ‡∏ü‡πâ‡∏≤ = ‡∏ß‡∏±‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥, ‡∏à‡∏∏‡∏î‡∏™‡∏µ‡πÅ‡∏î‡∏á = ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏™‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ + 2*std)")

        # ‡πÉ‡∏ä‡πâ st.scatter_chart ‡∏ó‡∏≥‡∏™‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô (‡∏´‡∏£‡∏∑‡∏≠ Nat ‡∏à‡∏∞‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Altair ‡∏Å‡πá‡πÑ‡∏î‡πâ)
        st.scatter_chart(
            normal_days,
            x="date_dt",
            y="count",
        )
        st.scatter_chart(
            anomaly_days,
            x="date_dt",
            y="count",
        )
        st.caption("‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ù‡∏ô‡∏ï‡∏Å‡∏´‡∏ô‡∏±‡∏Å, ‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°, ‡πÑ‡∏ü‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏ç‡πà ‡∏Ø‡∏•‡∏Ø")

        st.markdown("#### ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡∏±‡∏ô‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥")
        st.dataframe(
            anomaly_days[["date", "count", "roll_mean_7", "roll_std_7"]]
            .sort_values("date")
        )
